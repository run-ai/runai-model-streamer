{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream multiple Safetensors files To GPU\n",
    "\n",
    "In this notebook we will demonstrate how to read model tensors that are divided to multiple files in parallel using the RunAI Model Streamer and copy them to the GPU memory.\n",
    "\n",
    "## Prerequisite\n",
    "Run this notebook on a Linux machine with GPU.\n",
    "\n",
    "## Preperation\n",
    "We will start by downloading few example `.safetensors` files. Feel free to use your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "url = 'https://huggingface.co/vidore/colpali/resolve/main/adapter_model.safetensors?download=true'\n",
    "local_filename_1 = 'model-1.safetensors'\n",
    "\n",
    "wget_command = ['wget', '--content-disposition', url, '-O', local_filename_1]\n",
    "subprocess.run(wget_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "url = 'https://huggingface.co/boltuix/NeuroBERT-Mini/resolve/main/model.safetensors?download=true'\n",
    "local_filename_2 = 'model-2.safetensors'\n",
    "\n",
    "wget_command = ['wget', '--content-disposition', url, '-O', local_filename_2]\n",
    "subprocess.run(wget_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "To load the tensors from the files we need to create `SafetensorsStreamer` instance, perform the request, and transfer the tensors to the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runai_model_streamer import SafetensorsStreamer\n",
    "\n",
    "file_paths = [\"model-1.safetensors\", \"model-2.safetensors\"]\n",
    "\n",
    "with SafetensorsStreamer() as streamer:\n",
    "    streamer.stream_files(file_paths)\n",
    "    for name, tensor in streamer.get_tensors():\n",
    "        gpu_tensor = tensor.to('CUDA:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each yielded tensor is copied to the GPU, while in the background the streamer continues to read the next tensors. Therefore, reading from storage and copying to GPU are performed in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
